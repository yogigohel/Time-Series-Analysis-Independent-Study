**Definition 1.6: Strictly Stationary Time Series**
> A strictly stationary time series is one for which the probabilistic behavior of every collection of values $\{x_{t_1},...,x_{t_k}\}$ is identical to that of the time shifted set $\{x_{t_1+h},...,x_{t_k+h}\}$.
> 
> This would mean $\Pr\{x_{t_1} \leq c_1, ..., x_{t_k} \leq c_k\} = \Pr\{x_{t_1+h} \leq c_1, ..., x_{t_k+h} \leq c_k\}$ for all $k$, all time points $t_1, ..., t_k$, all numbers $c_1, ..., c_k$, and all time shifts $h=\pm 0,1,...$.
> 
> If a time series is strictly stationary, then all of the multivariate distribution functions for subsets of variables must agree with their counterparts in the shifted set for all values of the shift parameter $h$.
> > Example: When $k=2$, we can say $\Pr\{x_s\leq c_1, s_t\leq c_2\}=\Pr\{x_{s+h}\leq c_1, x_{t+h}\leq c_2\}$ for any $s,t$ and shift $h$. Also, if the variance exists here, then the autocovariance function of the series $x_t$ satisfies $\gamma(s,t)=\gamma(s+h,t+h)$ for all $s,t,h$. (Basically the autocovariance only depends on the time difference between $s$ and $t$, not their actually values).

**Definition 1.7: Weakly Stationary Time Series**
> A weakly stationary time series, $x_t$, is a finite variance process such that
> > 1.) The mean value function, $\mu_t$, is constant and does not depend on time $t$ and
> > 2.) The autocovariance function, $\gamma(s, t)$, depends only on the difference $|s-t|$ and not the actual values of $s$ and $t$.
> 
> We use the term stationary to mean weakly stationary for the rest of the text.
> 
> Stationarity requires regularity in the mean and autocorrelation functions so that these quantities (at least) may be estimated by averaging
> 
> A strictly stationary time series with finite variance is stationary.

**Definition 1.8**
> The autocovariance function of a stationary time series will be written as $\gamma(h) = cov(x_{t+h},x_t) = E[(x_{t+h}-\mu)(x_t-\mu)]$ because its only dependent on the time shift (lag). (We should be able to pick an arbitrary $t$ in this formula).

**Definition 1.9**
>The autocorrelation function (ACF) of a stationary time series will be written using $\rho(h)=\frac{\gamma(t+h,t)}{\sqrt{\gamma(t+h,t+h)\gamma(t,t)}}= \frac{\gamma(h)}{\gamma(0)}$.


#### Example 1.19: White Noise is Stationary
$$\gamma_w(h) = cov(w_{t+h}, w_t) = \cases{\sigma_w^2 \quad h=0 \\ 0 \quad\ \ h\neq0}$$If the white noise variates are also normally distributed or Gaussian, the series is also strictly stationary


#### Example 1.20: Moving Average is Stationary
From example 1.13 and 1.17, we know $$\gamma_v(h) = \cases{\frac{3}{9}\sigma_w^2 \quad h=0 \\ \frac{2}{9}\sigma_w^2 \quad h=\pm1 \\ \frac{1}{9}\sigma_w^2 \quad h=\pm2 \\ 0 \quad \quad |h|\gt 2}$$ and $$\rho_v(h) = \cases{1 \quad h=0 \\ \frac{2}{3} \quad h=\pm1 \\ \frac{1}{3} \quad h=\pm2 \\ 0 \quad \quad |h|\gt 2}$$

#### Example 1.21: Random Walk is not Stationary
It is not stationary because the autocovariance function $\gamma(s,t) = \min(s,t)\sigma_w^2$ depends on time. Also, the random walk with drift violates the first rule of stationary since $\mu_{xt}=\delta t$ which is a function of time.

#### Example 1.22: Trend Stationary
When a time series has a autocovariance function that is not dependent on time but the mean function is dependent on time, then the time series is said to have trend stationarity.

#### Properties of Stationary Time Series
> 1.) $\gamma(h)$ is non-negative definite so variances of linear combinations of the variates $x_t$ will never be negative. That is $0 \geq var(a_1x_1+...+a_nx_n)=\sum_{j=1}^n\sum_{k=1}^n a_ja_k\gamma(j-k)$.
> 2.) $\gamma(0) = E[(x_t - \mu)^2]$ is the variance of the time series.
> 3.) $\gamma(h) = \gamma(-h)$ for all $h$.

#### Notions of Stationarity with multiple series
