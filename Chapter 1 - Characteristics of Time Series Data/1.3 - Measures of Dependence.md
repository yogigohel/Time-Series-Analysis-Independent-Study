**Definition 1.1: Mean Function**
> The **mean function** is defined as $\mu_{xt} = E(x_t) = \int_{-\infty}^{\infty} xf_t(x)dx$, provided it exists, where $E$ denotes the usual expected value operator r. When no confusion exists about which time series we are referring to, we will drop a subscript and write $\mu_{xt}$ as $\mu_t$.

**Definition 1.2: The Autocovariance Function**
> The autocovariance function is defined as the second moment product $\gamma_x(s,t) = \text{cov}(x_s,x_t) = E[(x_s-\mu_s)(x_t-\mu_t)]$, for all $s$ and $t$. When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\gamma_x(s,t)$ and $\gamma_y(s,t)$ for all time points $s$ and $t$.

**Importance of Autocovariance**
> The autocovariance measures the linear dependence between two points on the same series observed at different times.
> Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart.
> Choppy series tend to have autocovariance functions that are nearly zero for large separations.
> If $\gamma_x(s,t) = 0$, $x_s$ and $x_t$ are not linearly related, but there still may be some dependence structure between them.
> If, however, $x_s$ and $x_t$ are bivariate normal, $\gamma_x (s, t) = 0$ ensures their independence.

**Example 1.16:**
