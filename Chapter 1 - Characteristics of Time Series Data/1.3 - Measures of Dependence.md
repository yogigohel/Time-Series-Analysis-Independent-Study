In order to get a complete description of a time series with $n$ random variables observed over $t_1, ..., t_n$ time points, we should know the following joint cdf: $$F_{t_1,...,t_n}(c_1,...,c _n) = \Pr(x_1 \leq c_1, ..., x_n \leq c_n)$$However, this is usually very ugly and impossible to plot and analyze time series. So, we look at the marginal distribution functions $F_t(x) = \Pr(x_t \leq x)$ or the marginal density functions $f_t(x) = \frac{\partial F_t(x)}{\partial x}$ when they exist. 


**Definition 1.1: Mean Function**
> The **mean function** is defined as $\mu_{xt} = E(x_t) = \int_{-\infty}^{\infty} xf_t(x)dx$, provided it exists, where $E$ denotes the usual expected value operator r. When no confusion exists about which time series we are referring to, we will drop a subscript and write $\mu_{xt}$ as $\mu_t$.
> 
> The mean function is also another important marginal descriptive measure.


**Definition 1.2: The Autocovariance Function**
> The autocovariance function is defined as the second moment product $\gamma_x(s,t) = \text{cov}(x_s,x_t) = E[(x_s-\mu_s)(x_t-\mu_t)]$, for all $s$ and $t$. When no possible confusion exists about which time series we are referring to, we will drop the subscript and write $\gamma_x(s,t)$ and $\gamma_y(s,t)$ for all time points $s$ and $t$.


**Importance of Autocovariance**
> The autocovariance measures the linear dependence between two points on the same series observed at different times.
> Very smooth series exhibit autocovariance functions that stay large even when the t and s are far apart.
> Choppy series tend to have autocovariance functions that are nearly zero for large separations.
> If $\gamma_x(s,t) = 0$, $x_s$ and $x_t$ are not linearly related, but there still may be some dependence structure between them.
> If, however, $x_s$ and $x_t$ are bivariate normal, $\gamma_x (s, t) = 0$ ensures their independence.


#### Example 1.17: Autocovariance of Moving Average
![[1.3 Autocov of Mov Avg pt 1.png]]
![[1.3 Autocov of Mov Avg pt 2.png]]

#### Example 1.18: Autocovariance of Random Walk 
![[1.3 Autocov of Rand Walk.png]]


Since autocovariance is not bounded (think example 1.18), we sometimes want a measure of association that is bounded like the following:

**Definition 1.3: The Autocorrelation Function**
> The autocorrelation function is defined as $\rho(s,t)=\frac{\gamma(s,t)}{\sqrt{\gamma(s,s)\gamma(t,t)}}$.
> ACF measures the linear predictability of the series at time t, so $x_t$, using only the value $x_s$.
> If we can predict $x_t$ perfectly from $x_s$ through a linear relationship, $x_t = \beta_0 + \beta_1 x_s$, then $\rho(s,t)=\cases{+1 \quad \beta_1>0 \\ -1 \quad \beta_1 < 0}$.


When we want to measure the the predictability of another series $y_t$ from the series $x_s$ (and both series have finite variances), we consider the following definitions:

**Definition 1.4: Cross-Covariance Function**
> The cross-covariance function between two series $x_t$ and $y_t$ is $\gamma_{x,y}(s,t) = cov(x_s,y_t) = E[(x_s-\mu_{xs})(y_t-\mu_{yt})]$.

Think about how we might use the Dow Jones data from 3 days ago to measure the NASDAQ 2 days from now. So this is 2 different series and 2 different time points.

**Definition 1.5: Cross-Correlation Function**
> The cross-correlation function (CCF) is given by $\rho_{x,y}(s,t)=\frac{\gamma_{x,y}(s,t)}{\sqrt{\gamma_x(s,s)\gamma_y(t,t)}}$.


The above ideas can be extended to the multiple series.

In the definitions above, the autocovariance and cross-covariance functions may change as one moves along the series because the values depend on both $s$ and $t$, the locations of the points in time.

When the autocovariance function only depends on the seperation of time (only on $s-t$ not what values of $s$ and $t$ are) and has constant mean, the stochastic process is called **weak stationary**.